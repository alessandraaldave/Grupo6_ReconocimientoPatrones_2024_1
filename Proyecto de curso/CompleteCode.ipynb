{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leIkL1Qr7Z8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9df7785-eb1f-4d7a-e547-c5be008a76d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aumento de datos**"
      ],
      "metadata": {
        "id": "Hb_R4qMUHQKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import random\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "FCT4rzqoV7Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraer nombre de las carpetas"
      ],
      "metadata": {
        "id": "M3Aa3MTCHDfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('imagenet_vid_train_15frames.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "\n",
        "# Extraer los nombres de las carpetas y añadir \"/content/\" al principio de cada uno\n",
        "folder_names = [\"/content/\" + video[\"name\"] for video in data[\"videos\"]]\n",
        "\n",
        "significant_frames1 = []\n",
        "\n",
        "for video in data[\"videos\"]:\n",
        "    frames = video[\"vid_train_frames\"]\n",
        "    frames = [frame for frame in frames if frame != -1]  # Quitar -1\n",
        "    significant_frames1.append(frames)\n",
        "\n",
        "\n",
        "#print(folder_names)\n",
        "\n",
        "# Imprimir las listas en sublistas de una más grande\n",
        "#print(significant_frames1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TjK4jRnG8nM",
        "outputId": "d8fb6d1a-e726-4800-8def-d47b03c1fdd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/benign/x28f299ceb056964c', '/content/benign/7de856d2db6d4700', '/content/benign/x717d46eb599764e', '/content/malignant/355f8098c8aa52ed', '/content/malignant/440c1dc384492d08', '/content/benign/x4771d87a9fdeb83b', '/content/benign/x32a4159934e524d5', '/content/benign/666818cd5f46c701', '/content/malignant/x27a0646da982181b', '/content/malignant/58534f86429d0af9', '/content/malignant/x460c43be47ad80b9', '/content/benign/531bdc37158304f9', '/content/malignant/x465f73e745809528', '/content/benign/6085475b648ee8f7', '/content/benign/32a6532065f51cec', '/content/malignant/4dc30d4e51ee2e48', '/content/benign/786ebd9dada09453', '/content/malignant/5b01151520267051', '/content/benign/x3748594d89375c56', '/content/benign/xff3777c79744514', '/content/malignant/x7cd4703a73a52918', '/content/malignant/69f4b1d2f756e60e', '/content/malignant/x53ce7f0e44720712', '/content/malignant/x2c7abb4741f95ad3', '/content/malignant/x5625e0d7d5570bc2', '/content/benign/x2be0b0e1c5d5aa61', '/content/malignant/30cd9711e8ddbf01', '/content/malignant/xb60099fde76c0ea', '/content/malignant/16a87e92f1b53338', '/content/malignant/xf7f83b2a576016b', '/content/malignant/x2ceef396ee055bee', '/content/malignant/2a2716116cf2a1da', '/content/benign/7c5574c3f669669f', '/content/malignant/3dd5fca83e9bb9ec', '/content/malignant/2c977dd2ec9d714a', '/content/malignant/x3206c740e53a558c', '/content/malignant/x5d893f3f3ae2bd22', '/content/benign/391ae00452fe2e99', '/content/malignant/x72392ccc925ed9d4', '/content/malignant/x22348897b96c20ff', '/content/benign/7db72602c876da5c', '/content/malignant/30945123ea4d9e62', '/content/benign/x4966baa34099fb85', '/content/benign/x5064979e8aa0aa5', '/content/malignant/38db199653d7170f', '/content/benign/60e246dd25972f25', '/content/malignant/37235fe6195d301a', '/content/malignant/x15ecb0b5256a4db', '/content/benign/6206535f64624442', '/content/malignant/x6178742ccbd3d1a0', '/content/benign/x63c9ba1377f35bf6', '/content/benign/x66ef02e7f1b9a0ef', '/content/malignant/x482b8dc6649140a0', '/content/benign/4414661bcb60d5cf', '/content/malignant/x6af4dbdbcc8cb3f8', '/content/malignant/x2cd35753cd695245', '/content/malignant/x2b8173ccbe4a0b7c', '/content/malignant/2aef523b046e4671', '/content/benign/x5872f6368719568a', '/content/malignant/758c7ef001885a88', '/content/benign/2cda21c3aab26332', '/content/benign/70a011e28ccdb8ee', '/content/malignant/3f88a121e5e77ebe', '/content/malignant/x2a61a821d462eeef', '/content/benign/x703e370cbb212b94', '/content/malignant/x1296b567ca4aceef', '/content/malignant/x59e4ed2abb496371', '/content/benign/61f569f918fa4e91', '/content/benign/dd4a021723b952b', '/content/malignant/x75bf1ba3e360f880', '/content/malignant/x43ddd60b2f249f9c', '/content/malignant/76070c32f89abc62', '/content/benign/263b86b85a58f270', '/content/malignant/x4bb046844e002870', '/content/benign/x3447a58f43172bcf', '/content/benign/x5a1c46ec6377e946', '/content/benign/63f67a1e6fb00f62', '/content/malignant/x48efc0d10583c373', '/content/malignant/x3b88488853e8b7d1', '/content/benign/x7d959113466b09ea', '/content/malignant/x237b754340100e54', '/content/malignant/x3b71e7e2f2960f1', '/content/malignant/3b9d9cbd7ac0825d', '/content/malignant/1dc9ca2f1748c2ec', '/content/malignant/x56fa67202af8fc78', '/content/benign/x15b845abcccc6f4a', '/content/benign/x54e1c05fd2287965', '/content/benign/x2c53d7998b5cc9f', '/content/malignant/x489d6fb55d2f0cd1', '/content/malignant/xe60a9301eb8c62f', '/content/malignant/x32b1b5654356c961', '/content/malignant/5890d06909bab23c', '/content/benign/x5884ac9e245060c7', '/content/malignant/x52bec9603c60ab38', '/content/benign/x4cdeb03e350b6747', '/content/malignant/x30fdb049d2060a3f', '/content/malignant/61663866db6802ce', '/content/malignant/129dded7559f99cf', '/content/malignant/x30c9d9bb3f0c6bff', '/content/malignant/x534a61eb0a51857e', '/content/malignant/49f974ba579679df', '/content/benign/x6ad6e92a9f2d2bc', '/content/malignant/x1feba2eda64ae560', '/content/malignant/7dfdb91ee9d7eb69', '/content/benign/670e132749ce2925', '/content/benign/x54e63696be637e25', '/content/benign/6146d3ec3285a7b7', '/content/malignant/46127d08b575d72', '/content/malignant/x545a3eb7bc929158', '/content/malignant/2f60c46643464ab5', '/content/benign/x2eb5d4310d6b7838', '/content/benign/x7177931c21f40518', '/content/benign/x63d5f68df3e660df', '/content/malignant/x38f36704318f33c2', '/content/benign/xe241f9d440204eb', '/content/malignant/x20aad08ffd1b00fe', '/content/malignant/e3503755fd4d348', '/content/benign/x2b276529233e7007', '/content/malignant/563cc92411e88f77', '/content/benign/x4bea72fdbc93f2f0', '/content/malignant/x6ef003070f181b71', '/content/benign/x6c31bd41dec5a8e3', '/content/malignant/2e374bbb7be3cf6a', '/content/malignant/6f27d394645c00f8', '/content/benign/x6f7af0c0342a44ed', '/content/malignant/x46d9ac246dbda50', '/content/benign/x1282311c38f808f', '/content/malignant/44ee98288ea9f1f4', '/content/benign/x671a7cb526b42548', '/content/malignant/x3d153e5f5b0bcde5', '/content/benign/bf669588716546e', '/content/malignant/x71559b11320fd2a', '/content/benign/x5d28852e092c471b', '/content/malignant/6e961391d009a706', '/content/malignant/x72853b7ced75b4fb', '/content/benign/c6245b90cc6db38', '/content/benign/x39f4ca5252085d82', '/content/malignant/x70ed0f8509d73ff0', '/content/benign/xca03e9925e2f762', '/content/malignant/x6862f478fdf97441', '/content/malignant/4d4db26cc9dd4024', '/content/benign/x31cbb3faa02b30cb', '/content/malignant/59ed378fcabf125d', '/content/malignant/x26b7abde2fb90d84', '/content/malignant/3448ca28ea37c2fa', '/content/benign/x63b732fdded83b62', '/content/malignant/x6535d12c2a5aab9b', '/content/benign/5f053610403f7478', '/content/malignant/5acb28328e37ef57']\n",
            "[[9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139], [11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [14, 29, 44, 59, 74, 89, 104, 119, 134, 149, 164, 179, 194, 209], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [24, 49, 74, 99, 124, 149, 174, 199, 224, 249, 274, 299, 324, 349], [15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [19, 39, 59, 79, 99, 119, 139, 159, 179, 199, 219, 239, 259, 279], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83], [9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [14, 29, 44, 59, 74, 89, 104, 119, 134, 149, 164, 179, 194, 209], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167], [11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167], [22, 45, 68, 91, 114, 137, 160, 183, 206, 229, 252, 275, 298, 321], [14, 29, 44, 59, 74, 89, 104, 119, 134, 149, 164, 179, 194, 209], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83], [15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223], [15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [12, 25, 38, 51, 64, 77, 90, 103, 116, 129, 142, 155, 168, 181], [19, 39, 59, 79, 99, 119, 139, 159, 179, 199, 219, 239, 259, 279], [4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69], [11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [26, 53, 80, 107, 134, 161, 188, 215, 242, 269, 296, 323, 350, 377], [13, 27, 41, 55, 69, 83, 97, 111, 125, 139, 153, 167, 181, 195], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55], [11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167], [12, 25, 38, 51, 64, 77, 90, 103, 116, 129, 142, 155, 168, 181], [4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83], [25, 51, 77, 103, 129, 155, 181, 207, 233, 259, 285, 311, 337, 363], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [10, 21, 32, 43, 54, 65, 76, 87, 98, 109, 120, 131, 142, 153], [19, 39, 59, 79, 99, 119, 139, 159, 179, 199, 219, 239, 259, 279], [12, 25, 38, 51, 64, 77, 90, 103, 116, 129, 142, 155, 168, 181], [9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [10, 21, 32, 43, 54, 65, 76, 87, 98, 109, 120, 131, 142, 153], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 207, 223], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [14, 29, 44, 59, 74, 89, 104, 119, 134, 149, 164, 179, 194, 209], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83], [19, 39, 59, 79, 99, 119, 139, 159, 179, 199, 219, 239, 259, 279], [9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139], [11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167], [4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69], [12, 25, 38, 51, 64, 77, 90, 103, 116, 129, 142, 155, 168, 181], [11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [13, 27, 41, 55, 69, 83, 97, 111, 125, 139, 153, 167, 181, 195], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55], [4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [12, 25, 38, 51, 64, 77, 90, 103, 116, 129, 142, 155, 168, 181], [9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97], [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55], [16, 33, 50, 67, 84, 101, 118, 135, 152, 169, 186, 203, 220, 237], [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139], [14, 29, 44, 59, 74, 89, 104, 119, 134, 149, 164, 179, 194, 209], [8, 17, 26, 35, 44, 53, 62, 71, 80, 89, 98, 107, 116, 125], [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83], [12, 25, 38, 51, 64, 77, 90, 103, 116, 129, 142, 155, 168, 181], [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [16, 33, 50, 67, 84, 101, 118, 135, 152, 169, 186, 203, 220, 237], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55], [18, 37, 56, 75, 94, 113, 132, 151, 170, 189, 208, 227, 246, 265], [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55], [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27], [11, 23, 35, 47, 59, 71, 83, 95, 107, 119, 131, 143, 155, 167], [19, 39, 59, 79, 99, 119, 139, 159, 179, 199, 219, 239, 259, 279], [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definir las transformaciones"
      ],
      "metadata": {
        "id": "sKi-5-SmG-Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vertical_flip(image):\n",
        "    return cv2.flip(image, 0)\n",
        "\n",
        "def rotate(image, angle):\n",
        "    (h, w) = image.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    rotated = cv2.warpAffine(image, M, (w, h))  # Asegúrate de usar (w, h)\n",
        "    return rotated\n",
        "\n",
        "\n",
        "def translate(image, x, y):\n",
        "    M = np.float32([[1, 0, x], [0, 1, y]])\n",
        "    translated = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))  # Mantener las dimensiones originales\n",
        "    return translated\n",
        "\n",
        "\n",
        "def perspective_transform(image):\n",
        "    (h, w) = image.shape[:2]\n",
        "    src_points = np.float32([[0, 0], [w-2, 0], [0, h-3], [w-7, h-5]])\n",
        "    dst_points = np.float32([[0, 0], [w-8, 0], [0, h-2], [w-3, h-5]])  # Mantener los puntos originales\n",
        "    M = cv2.getPerspectiveTransform(src_points, dst_points)\n",
        "    warped = cv2.warpPerspective(image, M, (w, h))  # Mantener las dimensiones originales\n",
        "    return warped\n"
      ],
      "metadata": {
        "id": "ZVg1jFMNwuBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aumento de datos"
      ],
      "metadata": {
        "id": "8LEWu182HBUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para aplicar transformaciones a las imágenes\n",
        "def apply_transformations(image_folder):\n",
        "    # Lista de archivos de imagen, ordenada por nombre\n",
        "    image_files = sorted(glob.glob(os.path.join(image_folder, '*.png')))  # Cambia la extensión si es necesario\n",
        "\n",
        "    # Cargar las imágenes\n",
        "    images = [cv2.imread(image_file) for image_file in image_files]\n",
        "\n",
        "    # Definir ángulos y desplazamientos\n",
        "    angle = random.randint(90, 360)  # Ejemplo de rotación\n",
        "    x_translation = random.randint(10, 50)  # Ejemplo de traslación en x\n",
        "    y_translation = random.randint(10, 50)  # Ejemplo de traslación en y\n",
        "\n",
        "    # Aplicar las mismas transformaciones a todas las imágenes\n",
        "    transformed_images = []\n",
        "    for image in images:\n",
        "        img = vertical_flip(image)\n",
        "        img = rotate(img, angle)\n",
        "        img = translate(img, x_translation, y_translation)\n",
        "        img = perspective_transform(img)\n",
        "        transformed_images.append(img)\n",
        "\n",
        "    # Crear una carpeta para guardar las imágenes transformadas\n",
        "    output_folder = os.path.join('/content/transformed_images', os.path.basename(image_folder))\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Guardar las imágenes transformadas\n",
        "    for i, img in enumerate(transformed_images):\n",
        "        cv2.imwrite(os.path.join(output_folder, f'image_{i}.png'), img)\n",
        "\n",
        "    # Comprimir la carpeta\n",
        "    shutil.make_archive(output_folder, 'zip', output_folder)\n",
        "\n",
        "    # Descargar la carpeta comprimida\n",
        "    files.download(f'{output_folder}.zip')\n",
        "\n",
        "# Iterar sobre cada carpeta en folder_names y aplicar las transformaciones\n",
        "for folder_name in folder_names:\n",
        "    image_folder = os.path.join(folder_names)\n",
        "    apply_transformations(image_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Qm4J-WA0xQx5",
        "outputId": "4db84a81-5245-46a1-bb54-c33ab52c589a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9b3b9df5-bf3c-45b3-aee4-d08b8921a3d9\", \"transformed_images.zip\", 30965571)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para unir todas las carpetas de imágenes"
      ],
      "metadata": {
        "id": "tD_Y27VRLKbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Directorio de interés\n",
        "directory = \"/content/transformed_images\"\n",
        "\n",
        "# Iterar sobre los elementos en el directorio\n",
        "for item in os.listdir(directory):\n",
        "    # Comprobar si es una carpeta y si comienza con '/content/transformed_images'\n",
        "    if os.path.isdir(os.path.join(directory, item)):\n",
        "        # Agregar el nombre de la carpeta a la lista\n",
        "        folder_names.append('/content/transformed_images/'+ item)\n",
        "\n",
        "# print(folder_names)\n",
        "\n",
        "significant_frames1 = significant_frames1*2\n",
        "#print(significant_frames1)\n",
        "\n",
        "# Transformar la lista\n",
        "significant_frames = []\n",
        "\n",
        "for sublista in significant_frames1:\n",
        "    nueva_sublista = [f\"{num:06}.png\" for num in sublista]\n",
        "    significant_frames.append(nueva_sublista)\n"
      ],
      "metadata": {
        "id": "hF2P7ROtQYU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Algortimo CNN**"
      ],
      "metadata": {
        "id": "VJryYWrlHXxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesamiento de datos"
      ],
      "metadata": {
        "id": "wZ7feDORHduG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "def load_images_from_folders_with_significant_frames(folder_names, significant_frames):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Recorrer las carpetas y sus correspondientes frames significativos\n",
        "    for folder, sig_frames in zip(folder_names, significant_frames):\n",
        "        if os.path.isdir(folder):\n",
        "            for filename in os.listdir(folder):\n",
        "                img_path = os.path.join(folder, filename)\n",
        "                if os.path.isfile(img_path):\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is not None:\n",
        "                        # Redimensionar la imagen si es necesario\n",
        "                        img = cv2.resize(img, (224, 224))  # Ajusta este tamaño según sea necesario\n",
        "                        img = img_to_array(img)\n",
        "                        images.append(img)\n",
        "                        # Comprobar si el nombre del frame está en la lista de frames significativos\n",
        "                        label = 1 if filename in sig_frames else 0\n",
        "                        labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "\n",
        "# Cargar imágenes y etiquetas\n",
        "X, y = load_images_from_folders_with_significant_frames(folder_names, significant_frames)\n"
      ],
      "metadata": {
        "id": "D_oec2ndHHFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.astype('float32') / 255.0\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verificar dimensiones\n",
        "print(\"Tamaño del conjunto de entrenamiento:\", X_train.shape)\n",
        "print(\"Tamaño del conjunto de prueba:\", X_test.shape)"
      ],
      "metadata": {
        "id": "65ZPzeBALuH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construcción del modelo"
      ],
      "metadata": {
        "id": "NEIkLkHRKKPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models, callbacks\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suponiendo que X_train, y_train, X_test, y_test ya están definidos y preprocesados\n",
        "\n",
        "# Obtener las dimensiones de la primera imagen\n",
        "altura, anchura, canales = X_train[0].shape\n",
        "\n",
        "# Paso 2: Construcción del modelo\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(altura, anchura, canales)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Salida binaria: frame significativo o no\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Paso 3: Configuración de callbacks\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "model_checkpoint = callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Entrenar el modelo y guardar el historial de entrenamiento\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test),\n",
        "                    callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# El modelo ya tiene los mejores pesos debido a restore_best_weights=True en EarlyStopping\n",
        "\n",
        "# Paso 4: Evaluación del modelo\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
        "\n",
        "print(classification_report(y_test, y_pred_binary))"
      ],
      "metadata": {
        "id": "IO97EzmEKFe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Obtener las métricas del historial de entrenamiento\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Crear una figura con dos subgráficos\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Subgráfico para la pérdida\n",
        "ax1.plot(train_loss, label='Training Loss', color='blue')\n",
        "ax1.plot(val_loss, label='Validation Loss', color='red')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Subgráfico para la precisión\n",
        "ax2.plot(train_accuracy, label='Training Accuracy', color='blue')\n",
        "ax2.plot(val_accuracy, label='Validation Accuracy', color='red')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Mostrar la figura\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sU9-8q-gKGcX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}